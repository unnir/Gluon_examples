{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd, ndarray\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "data = load_boston()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data for the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm = (df - df.mean()) / (df.max() - df.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.040322</td>\n",
       "      <td>0.066364</td>\n",
       "      <td>-0.323562</td>\n",
       "      <td>-0.06917</td>\n",
       "      <td>-0.034352</td>\n",
       "      <td>0.055636</td>\n",
       "      <td>-0.034757</td>\n",
       "      <td>0.026822</td>\n",
       "      <td>-0.371713</td>\n",
       "      <td>-0.214193</td>\n",
       "      <td>-0.335695</td>\n",
       "      <td>0.101432</td>\n",
       "      <td>-0.211729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.040086</td>\n",
       "      <td>-0.113636</td>\n",
       "      <td>-0.149075</td>\n",
       "      <td>-0.06917</td>\n",
       "      <td>-0.176327</td>\n",
       "      <td>0.026129</td>\n",
       "      <td>0.106335</td>\n",
       "      <td>0.106581</td>\n",
       "      <td>-0.328235</td>\n",
       "      <td>-0.317246</td>\n",
       "      <td>-0.069738</td>\n",
       "      <td>0.101432</td>\n",
       "      <td>-0.096939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.040086</td>\n",
       "      <td>-0.113636</td>\n",
       "      <td>-0.149075</td>\n",
       "      <td>-0.06917</td>\n",
       "      <td>-0.176327</td>\n",
       "      <td>0.172517</td>\n",
       "      <td>-0.076981</td>\n",
       "      <td>0.106581</td>\n",
       "      <td>-0.328235</td>\n",
       "      <td>-0.317246</td>\n",
       "      <td>-0.069738</td>\n",
       "      <td>0.091169</td>\n",
       "      <td>-0.237943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.040029</td>\n",
       "      <td>-0.113636</td>\n",
       "      <td>-0.328328</td>\n",
       "      <td>-0.06917</td>\n",
       "      <td>-0.198961</td>\n",
       "      <td>0.136686</td>\n",
       "      <td>-0.234551</td>\n",
       "      <td>0.206163</td>\n",
       "      <td>-0.284757</td>\n",
       "      <td>-0.355414</td>\n",
       "      <td>0.026007</td>\n",
       "      <td>0.095708</td>\n",
       "      <td>-0.268021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.039617</td>\n",
       "      <td>-0.113636</td>\n",
       "      <td>-0.328328</td>\n",
       "      <td>-0.06917</td>\n",
       "      <td>-0.198961</td>\n",
       "      <td>0.165236</td>\n",
       "      <td>-0.148042</td>\n",
       "      <td>0.206163</td>\n",
       "      <td>-0.284757</td>\n",
       "      <td>-0.355414</td>\n",
       "      <td>0.026007</td>\n",
       "      <td>0.101432</td>\n",
       "      <td>-0.202071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CRIM        ZN     INDUS     CHAS       NOX        RM       AGE  \\\n",
       "0 -0.040322  0.066364 -0.323562 -0.06917 -0.034352  0.055636 -0.034757   \n",
       "1 -0.040086 -0.113636 -0.149075 -0.06917 -0.176327  0.026129  0.106335   \n",
       "2 -0.040086 -0.113636 -0.149075 -0.06917 -0.176327  0.172517 -0.076981   \n",
       "3 -0.040029 -0.113636 -0.328328 -0.06917 -0.198961  0.136686 -0.234551   \n",
       "4 -0.039617 -0.113636 -0.328328 -0.06917 -0.198961  0.165236 -0.148042   \n",
       "\n",
       "        DIS       RAD       TAX   PTRATIO         B     LSTAT  \n",
       "0  0.026822 -0.371713 -0.214193 -0.335695  0.101432 -0.211729  \n",
       "1  0.106581 -0.328235 -0.317246 -0.069738  0.101432 -0.096939  \n",
       "2  0.106581 -0.328235 -0.317246 -0.069738  0.091169 -0.237943  \n",
       "3  0.206163 -0.284757 -0.355414  0.026007  0.095708 -0.268021  \n",
       "4  0.206163 -0.284757 -0.355414  0.026007  0.101432 -0.202071  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_norm, y,\n",
    "                                                    test_size=0.2, random_state=1111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gluon Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LEARNING_R = 0.01\n",
    "EPOCHS = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = mx.gluon.data.ArrayDataset(X_train.as_matrix(),y_train)\n",
    "test_dataset = mx.gluon.data.ArrayDataset(X_test.as_matrix(),y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = mx.gluon.data.DataLoader(train_dataset,\n",
    "                                      batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_data = mx.gluon.data.DataLoader(test_dataset,\n",
    "                                     batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = gluon.nn.Sequential()\n",
    "\n",
    "# Define the model architecture\n",
    "with net.name_scope(): \n",
    "    net.add(gluon.nn.Dense(16, activation=\"relu\") ) \n",
    "    net.add(gluon.nn.BatchNorm())    \n",
    "    net.add(gluon.nn.Dense(8, activation=\"relu\") ) \n",
    "    net.add(gluon.nn.BatchNorm())    \n",
    "    net.add(gluon.nn.Dense(1))\n",
    "\n",
    "# Intitalize parametes of the model\n",
    "net.collect_params().initialize(mx.init.Uniform())\n",
    "\n",
    "# Add binary loss function\n",
    "l2loss = gluon.loss.L2Loss()\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': LEARNING_R})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Current Loss: 245.78768920898438.\n",
      "Epoch 20. Current Loss: 7.980735778808594.\n",
      "Epoch 40. Current Loss: 4.9582600593566895.\n",
      "Epoch 60. Current Loss: 6.980565547943115.\n",
      "Epoch 80. Current Loss: 0.960381031036377.\n",
      "Epoch 100. Current Loss: 7.814993381500244.\n",
      "Epoch 120. Current Loss: 9.498841285705566.\n",
      "Epoch 140. Current Loss: 17.053531646728516.\n",
      "Epoch 160. Current Loss: 5.0172576904296875.\n",
      "Epoch 180. Current Loss: 7.0766472816467285.\n",
      "Epoch 200. Current Loss: 2.061584949493408.\n",
      "Epoch 220. Current Loss: 4.6772260665893555.\n",
      "Epoch 240. Current Loss: 4.503413200378418.\n",
      "Epoch 260. Current Loss: 6.620635986328125.\n",
      "Epoch 280. Current Loss: 3.283313751220703.\n"
     ]
    }
   ],
   "source": [
    "for e in range(EPOCHS):\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(mx.cpu()).astype('float32')\n",
    "        label = label.as_in_context(mx.cpu()).astype('float32')\n",
    "        with autograd.record(): # Start recording the derivatives\n",
    "            output = net(data) # the forward iteration\n",
    "            loss = l2loss(output, label)\n",
    "            loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        # Provide stats on the improvement of the model over each epoch\n",
    "        curr_loss = ndarray.mean(loss).asscalar()\n",
    "    if e % 20 == 0:\n",
    "        print(\"Epoch {}. Current Loss: {}.\".format(e, curr_loss))\n",
    "#     if curr_loss < 2.0:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = np.array([])\n",
    "for data,label in test_data:\n",
    "        data = data.as_in_context(mx.cpu()).astype('float32')\n",
    "        label = label.as_in_context(mx.cpu()).astype('float32')\n",
    "        output = net(data)\n",
    "        y_pred = np.append(y_pred, output.asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.9256329511598826"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
